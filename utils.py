from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import wave


def get_wav_duration(fname):
    """
    Get the duration of a .wav audio file.
    """
    f = wave.open(fname, 'r')
    frames = f.getnframes()
    rate = f.getframerate()
    duration = frames / float(rate)
    f.close()
    return duration


def get_trans_content(file_path):
    """
    Open and read the content in file_path.
    """
    content = None
    if file_path.exists():
        with open(file_path, 'r') as fin:
            content = ' '.join(fin.readlines())
    return content


def gen_audio_info_tab(out_fname='audio_info_table.csv', verbose=False):
    """
    Generate a table containing information of audio files from the
    user study. Each record is identified by a shortened user ID, recording
    time of the audio file, and the path to the audio file.
    """
    path_data_dir = 'data'
    glob_maindata_dir = 'UserStudy*'
    glob_audio_fs = 'MergedMicAudio*.wav'
    glob_log_file = '*Log.txt'
    records = []

    # For all files in the "data" folder
    data_dir = Path(path_data_dir)
    for ustudy_dir in data_dir.glob(glob_maindata_dir):

        if not ustudy_dir.is_dir():
            continue

        # For all files in each user study folder
        for user_dir in ustudy_dir.glob("*"):

            if not user_dir.is_dir():
                continue

            # Extract the user_id and group_id
            user_id = user_dir.name
            group_id = ustudy_dir.name

            # Generate the shortened ID
            _tmp = user_dir.stem.split('-')
            short_id = _tmp[0][0] + _tmp[1][0] + '-' + ''.join(_tmp[2:])

            # Find the log file
            log_path = list(user_dir.glob(glob_log_file))

            # Iterate all audio files and mark user IDs
            for audio_path in user_dir.glob(glob_audio_fs):

                # .txt files are transcriptions generated by GCloud service
                txt_path = Path(str(audio_path) + ".txt")
                txt_content = get_trans_content(txt_path)
                # .r.txt files are transcription made by myself
                txt_r_path = Path(str(audio_path) + ".r.txt")
                txt_r_content = get_trans_content(txt_r_path)

                # Audio starting timestamp
                audio_st_time = ''.join(
                    audio_path.stem.split('-')[1:]).replace('.wav', '')

                # Append to the record list (which will be exported as a dataframe)
                records.append((
                    user_id, group_id, short_id,
                    audio_st_time, str(audio_path),
                    str(txt_path), txt_content,
                    str(txt_r_path), txt_r_content,
                    str(log_path[0]),
                ))

                if verbose:
                    print("{}:{}:{}".format(
                        short_id, audio_st_time, audio_path))

    # Export as csv
    columns = (
        'user_id', 'group_id', 'short_id',
        'audio_st_time', 'audio_path',
        'txt_path', 'txt_content',
        'txt_r_path', 'txt_r_content',
        'log_path',
    )
    records = pd.DataFrame.from_records(records, columns=columns)
    records = records.sort_values(columns[1], ascending=True)
    records.to_csv(out_fname, index=None)

    return records


def read_task_log(path_fname, year='2019'):
    """
    Read the task log file into a dataframe. Only the first three columns in the log file are defined. The remaining columns are misc details.
    """
    path_fname = Path(path_fname)

    # Extract the group ID and user ID from the path
    group_id = path_fname.parent.parent.name
    user_id = path_fname.parent.name

    # Read and parse the task log file
    with open(path_fname, 'r') as fin:
        records = []
        for i, line in enumerate(fin.readlines()):
            cols = line.strip().split(', ')
            one_record = cols[:3] + [', '.join(cols[3:])]
            records.append(one_record)

    # Construct a dataframe
    df = pd.DataFrame.from_records(
        records, columns=['timestamp', 'log_class', 'log_tag', 'log_details'])

    # Update the timestamp
    df['timestamp'] = (year + '-' + df['timestamp']).str.replace('/', '-')

    # Append the user ID and group ID
    df['user_id'] = user_id
    df['group_id'] = group_id

    return df


def collect_task_logdata(data_path='data'):
    """
    Collect and aggregate all task log files in the user-study data folder.
    """
    print("[INFO] Start collecting task log files")
    print("[INFO] Iterating all user-study data folders...")
    dir_data = Path(data_path)
    task_log_records = []
    for dir_us in dir_data.glob('UserStudy*'):
        print("[INFO] Processing: {}".format(dir_us))
        log_files = dir_us.glob('*/*-Log.txt')

        for i in log_files:
            task_log_records.append(read_task_log(i))

    output_fname = 'task_log_records.csv'
    task_log_records = pd.concat(task_log_records)
    task_log_records.sort_values(['group_id', 'timestamp'], inplace=True)
    task_log_records.to_csv(output_fname, index=None)

    print('[INFO] Done. The result was saved in {}'.format(output_fname))

    return task_log_records


def read_code_snapshot(path_fname, year='2019'):
    """
    Read the given snapshot file and extract embedded meta info.
    """
    # Identify the group info and user ID
    user_id = path_fname.parent.name
    group_id = path_fname.parent.parent.name
    # basename will be like "PostEdit-ScriptSnapshot-11-25-13-06-58.txt"
    basename = path_fname.name.replace('.txt', '')
    split_basename = basename.split('-ScriptSnapshot-')
    # Extract the category (PostEdit or PreExec)
    sp_category = split_basename[0]
    # Extract the timestamp (the year has to be given)
    sp_timestamp = year + '-' + split_basename[1]
    sp_timestamp = datetime.strptime(sp_timestamp, "%Y-%m-%d-%H-%M-%S")
    # Read the content
    with open(path_fname, 'r') as fin:
        sp_content = fin.read()

    record = {
        'timestamp': sp_timestamp,
        'category': sp_category,
        'snapshot': sp_content,
        'user_id': user_id,
        'group_id': group_id,
        'path': path_fname,
    }
    return record


def collect_code_snapshots(data_path='data', snapshot_year='2019'):
    """
    Collect and aggregate all code snapshots in the user-study data folder.
    """
    print("[INFO] Start collecting code snapshots in task log data")

    print("[INFO] Iterating all user-study data folders...")
    dir_data = Path(data_path)
    code_snapshot_records = []
    for dir_us in dir_data.glob('UserStudy*'):
        print("[INFO] Processing: {}".format(dir_us))
        post_edit_snapshots = dir_us.glob('*/PostEdit-*')
        pre_exec_snapshots = dir_us.glob('*/PreExec-*')

        for i in post_edit_snapshots:
            code_snapshot_records.append(
                read_code_snapshot(i, year=snapshot_year))
        for i in pre_exec_snapshots:
            code_snapshot_records.append(
                read_code_snapshot(i, year=snapshot_year))

    # Convert into dataframe and then export
    output_fname = 'code_snapshot_records.csv'
    code_snapshot_df = pd.DataFrame.from_records(code_snapshot_records)
    code_snapshot_df.sort_values(['group_id', 'timestamp'], inplace=True)
    code_snapshot_df.to_csv(output_fname, index=None)

    print('[INFO] Done. The result was saved in {}'.format(output_fname))

    return code_snapshot_df


if __name__ == "__main__":
    # code_snapshots = collect_code_snapshots()
    # task_log_records = collect_task_logdata()
    pass
